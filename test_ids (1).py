# -*- coding: utf-8 -*-
"""test_ids.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G1Glky1gFTmMa3ticGkYnYtEEMzJR6AC
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import os
import pickle
from tensorflow.keras.models import load_model

# === Helper Functions ===

def parse_can_message(line):
    try:
        line = line.strip()
        if not line or len(line) < 10:
            return None

        parts = line.split()
        timestamp = float(parts[1]) if parts[0] in ['Timestamp:', 'Time:'] else float(parts[0])
        can_id = int(parts[parts.index("ID:") + 1], 16) if "ID:" in parts else int(parts[1], 16)

        dlc_index = next((i for i, p in enumerate(parts) if p.upper() == "DLC:"), None)
        if dlc_index is None: return None
        dlc = int(parts[dlc_index + 1])

        data_start = dlc_index + 2
        if data_start < len(parts) and parts[data_start].upper() == "DATA:": data_start += 1

        data_bytes = []
        for i in range(min(dlc, 8)):
            try: data_bytes.append(int(parts[data_start + i], 16))
            except: data_bytes.append(0)
        while len(data_bytes) < 8: data_bytes.append(0)

        return [timestamp, can_id, dlc] + data_bytes
    except: return None

def extract_can_data(file_path):
    with open(file_path, 'r') as f:
        lines = f.readlines()
    return [parse_can_message(line) for line in lines if parse_can_message(line)]

def create_sequences(df, seq_len=20):
    features = ['ID', 'DLC'] + [f'D{i}' for i in range(8)] + ['Time_diff']
    if len(df) < seq_len:
        print(f"⚠️ Not enough messages for sequence length {seq_len}")
        return None
    return np.array([df[features].iloc[i:i+seq_len].values for i in range(len(df)-seq_len+1)])

def predict_attack(predictions):
    avg = np.mean(predictions, axis=0)
    pred_class = np.argmax(avg)
    labels = ['Normal', 'DoS', 'Fuzzy', 'Impersonation']
    one_hot = [1 if i == pred_class else 0 for i in range(4)]
    print("\n📊 Prediction Probabilities:")
    for i, name in enumerate(labels):
        print(f"{name:<15}: {avg[i]:.4f}")
    print(f"\n🎯 Predicted Class: {pred_class} ({labels[pred_class]})")
    return one_hot

# === MAIN ===

def main():
    scaler_path = "/content/drive/MyDrive/can/preprocessed/scaler_recreated.pkl"
    model_path = "/content/drive/MyDrive/can/lstm_model.keras"
    input_path = "/content/drive/MyDrive/can/input_D_2.txt"

    # Load scaler
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"✅ Loaded scaler from: {scaler_path}")

    # Load model
    model = load_model(model_path)
    print(f"✅ Loaded model from: {model_path}")

    # Load input
    data = extract_can_data(input_path)
    if len(data) < 20:
        print("❌ Not enough data to predict.")
        return

    df = pd.DataFrame(data, columns=['Time', 'ID', 'DLC'] + [f'D{i}' for i in range(8)])
    df = df.sort_values('Time').reset_index(drop=True)
    df['Time_diff'] = df['Time'].diff().fillna(0)

    features = ['ID', 'DLC'] + [f'D{i}' for i in range(8)] + ['Time_diff']
    df[features] = scaler.transform(df[features])

    sequences = create_sequences(df)
    if sequences is None: return

    predictions = model.predict(sequences, verbose=0)
    final_label = predict_attack(predictions)

    print(f"\n🏷️ Final Prediction Vector: {final_label}")

if __name__ == "__main__":
    main()

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import os
import pickle
from tensorflow.keras.models import load_model

# === Helper Functions ===

def parse_can_message(line):
    try:
        line = line.strip()
        if not line or len(line) < 10:
            return None

        parts = line.split()
        timestamp = float(parts[1]) if parts[0] in ['Timestamp:', 'Time:'] else float(parts[0])
        can_id = int(parts[parts.index("ID:") + 1], 16) if "ID:" in parts else int(parts[1], 16)

        dlc_index = next((i for i, p in enumerate(parts) if p.upper() == "DLC:"), None)
        if dlc_index is None: return None
        dlc = int(parts[dlc_index + 1])

        data_start = dlc_index + 2
        if data_start < len(parts) and parts[data_start].upper() == "DATA:": data_start += 1

        data_bytes = []
        for i in range(min(dlc, 8)):
            try: data_bytes.append(int(parts[data_start + i], 16))
            except: data_bytes.append(0)
        while len(data_bytes) < 8: data_bytes.append(0)

        return [timestamp, can_id, dlc] + data_bytes
    except: return None

def extract_can_data(file_path):
    with open(file_path, 'r') as f:
        lines = f.readlines()
    return [parse_can_message(line) for line in lines if parse_can_message(line)]

def create_sequences(df, seq_len=20):
    features = ['ID', 'DLC'] + [f'D{i}' for i in range(8)] + ['Time_diff']
    if len(df) < seq_len:
        print(f"⚠️ Not enough messages for sequence length {seq_len}")
        return None
    return np.array([df[features].iloc[i:i+seq_len].values for i in range(len(df)-seq_len+1)])

def predict_attack(predictions, save_path=None):
    avg = np.mean(predictions, axis=0)
    pred_class = np.argmax(avg)
    labels = ['Normal', 'DoS', 'Fuzzy', 'Impersonation']
    one_hot = [1 if i == pred_class else 0 for i in range(4)]

    print("\n📊 Prediction Probabilities:")
    for i, name in enumerate(labels):
        print(f"{name:<15}: {avg[i]:.4f}")
    print(f"\n🎯 Predicted Class: {pred_class} ({labels[pred_class]})")

    # === Save if path provided ===
    if save_path:
        # Save average probabilities
        pd.DataFrame([avg], columns=labels).to_csv(save_path, index=False)
        print(f"\n📁 Saved averaged prediction to: {save_path}")

        # Save all sequence predictions
        all_preds_path = save_path.replace("mean_", "all_")
        pd.DataFrame(predictions, columns=labels).to_csv(all_preds_path, index=False)
        print(f"📁 Saved all sequence predictions to: {all_preds_path}")

    return one_hot



# === MAIN ===

def main():
    scaler_path = "/content/drive/MyDrive/can/preprocessed/scaler_recreated.pkl"
    model_path = "/content/drive/MyDrive/can/lstm_model.h5"
    input_path = "/content/drive/MyDrive/can/input_A_2.txt"

    # Load scaler
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"✅ Loaded scaler from: {scaler_path}")

    # Load model
    model = load_model(model_path)
    print(f"✅ Loaded model from: {model_path}")

    # Load input
    data = extract_can_data(input_path)
    if len(data) < 20:
        print("❌ Not enough data to predict.")
        return

    df = pd.DataFrame(data, columns=['Time', 'ID', 'DLC'] + [f'D{i}' for i in range(8)])
    df = df.sort_values('Time').reset_index(drop=True)
    df['Time_diff'] = df['Time'].diff().fillna(0)

    features = ['ID', 'DLC'] + [f'D{i}' for i in range(8)] + ['Time_diff']
    df[features] = scaler.transform(df[features])

    sequences = create_sequences(df)
    if sequences is None: return

    predictions = model.predict(sequences, verbose=0)
    final_label = predict_attack(predictions)

    print(f"\n🏷️ Final Prediction Vector: {final_label}")
    final_label = predict_attack(predictions, save_path="/content/drive/MyDrive/can/mean_predictions.csv")

if __name__ == "__main__":
    main()